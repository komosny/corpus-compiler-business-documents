{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "!pip install -r requirements.txt --disable-pip-version-check | tail -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,sys,warnings,re,io,shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "import urllib.request\n",
    "import folium\n",
    "import requests\n",
    "\n",
    "from unidecode import unidecode\n",
    "from pypdf import PdfReader\n",
    "from contextlib import redirect_stderr\n",
    "from geopy.distance import great_circle \n",
    "from joblib import Memory\n",
    "from IPython.display import HTML,Image\n",
    "from requests_ratelimiter import LimiterSession\n",
    "from tqdm import tqdm\n",
    "from lingua import Language, LanguageDetectorBuilder, IsoCode639_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "os.makedirs('__cache/indexes', exist_ok=True)\n",
    "os.makedirs('__documents', exist_ok=True)\n",
    "os.makedirs('result-corpus', exist_ok=True) \n",
    "os.makedirs('result-figures', exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(location='__cache', verbose=0)\n",
    "\n",
    "tqdm.pandas(file=sys.stdout,ncols=100)\n",
    "\n",
    "lng_model = LanguageDetectorBuilder.from_languages(\n",
    "    Language.ENGLISH,Language.CZECH).with_preloaded_language_models().with_minimum_relative_distance(0.9).build()\n",
    "\n",
    "session = requests.Session()\n",
    "session_limited = LimiterSession(per_second=1)\n",
    "session_limited.headers.update({'User-Agent': 'research-data-bussiness',\n",
    "                                'email':'name@org.com',\n",
    "                                'Accept-Language': 'en-US'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST_API_GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def REST_API_GET(url):\n",
    "\n",
    "    response = pd.NA\n",
    "    \n",
    "    try:\n",
    "        response = session_limited.get(url=url)\n",
    "    except:\n",
    "        print('REST API connection error.')\n",
    "        raise SystemExit\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST_API_GET_LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def REST_API_GET_LOCAL(url):\n",
    "\n",
    "    response = pd.NA\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url=url)\n",
    "    except:\n",
    "        print('REST API connection error.')\n",
    "        raise SystemExit\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET_FILE_SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def GET_FILE_SOURCE(path):\n",
    "    \n",
    "    result = pd.NA\n",
    "\n",
    "    file_type = path.split('.')[-1]\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    if file_type == 'pdf':      \n",
    "\n",
    "        with io.StringIO() as buf, redirect_stderr(buf):\n",
    "        \n",
    "            try:\n",
    "                reader = PdfReader(path)\n",
    "                text = ''\n",
    "                \n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + '\\n'\n",
    "            \n",
    "                result = text\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET_LNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def GET_LNG(text):\n",
    "\n",
    "    result = pd.NA\n",
    "    \n",
    "    try:\n",
    "        lng = lng_model.detect_language_of(text).iso_code_639_1.name\n",
    "        result = lng\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geocode_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_addr(addr,raise_error_data=False,raise_error_request=False,try_nominatim_local=False):\n",
    "    \n",
    "    result = pd.NA\n",
    "\n",
    "    local_record = False\n",
    "\n",
    "    if try_nominatim_local:\n",
    "        try:\n",
    "            url = f'http://localhost:8080/search?q={addr}&addressdetails=1&format=json'\n",
    "            response = REST_API_GET_LOCAL(url=url)\n",
    "            response_data = json.loads(response.text)\n",
    "            #print('Local',url)\n",
    "            if len(response_data) > 0: local_record=True\n",
    "            #print(response_data)  \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not local_record:\n",
    "        url = f'https://nominatim.openstreetmap.org/search?q={addr}&addressdetails=1&format=json'\n",
    "        #print(url)\n",
    "        response = REST_API_GET(url=url)   \n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            response_data = json.loads(response.text)\n",
    "\n",
    "            #print(response.text)\n",
    "            #print(response_data)\n",
    "            #print(len(response_data))\n",
    "\n",
    "            if len(response_data) > 0:\n",
    "\n",
    "                # most relevant record\n",
    "                item = response_data[0]\n",
    "\n",
    "                lat = str(round(float(item['lat']),5))\n",
    "                lon = str(round(float(item['lon']),5))\n",
    "                bbox = item['boundingbox']\n",
    "                bbox = [str(round(float(item), 5)) for item in bbox]\n",
    "                \n",
    "                # rb lt crd\n",
    "                bbox = (bbox[0],bbox[2]),(bbox[1],bbox[3])\n",
    "\n",
    "                country = response_data[0]['address'].get('country_code',pd.NA)\n",
    "\n",
    "                road = item['address'].get('road',pd.NA)\n",
    "                square = item['address'].get('square',pd.NA)\n",
    "                # try in this order road/square\n",
    "                place = road\n",
    "                if pd.isna(place): place = square     \n",
    "                               \n",
    "                if pd.notna(country): country = unidecode(country).lower()\n",
    "                if pd.notna(place): place = unidecode(place).lower()\n",
    "\n",
    "                #result = (lat,lon),(bbox),municipality,country,len(response_data)\n",
    "                result = (lat,lon),(bbox),place,country\n",
    "\n",
    "        \n",
    "        except:\n",
    "            if raise_error_data:\n",
    "                print('REST API data error.')\n",
    "                raise SystemExit\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    else: \n",
    "        if raise_error_data:\n",
    "            print('REST API request error.')\n",
    "            raise SystemExit\n",
    "        else:\n",
    "            pass          \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url,local_path,use_cached=True,show=False):\n",
    "\n",
    "    result = pd.NA\n",
    "    \n",
    "    if use_cached==False or not os.path.exists(local_path) or os.path.getsize(local_path) == 0:        \n",
    "        try:\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                with open(local_path, 'wb') as file:\n",
    "                    file.write(response.read())\n",
    "                    if show: print(f'Downloaded {local_path}')\n",
    "                    result = local_path\n",
    "        except:\n",
    "            if show: print(f'Error {local_path}')        \n",
    "            pass\n",
    "    else:\n",
    "        result = local_path\n",
    "        if show: print(f'Stored {local_path}')\n",
    "      \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "           \n",
    "    text = unidecode(text)\n",
    "    text = re.findall(r'\\b[a-zA-Z0-9@,.-]+\\b',text)    \n",
    "    result = ' '.join(text)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse_archive_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_archive_files(archive_files):\n",
    "\n",
    "    result = pd.NA\n",
    "\n",
    "    if type(archive_files) is not list:\n",
    "        result = (archive_files.get('odkaz',pd.NA),)\n",
    "    else:        \n",
    "        files = tuple(pd.json_normalize(archive_files).odkaz.tolist())\n",
    "        \n",
    "        if len(files) > 0:\n",
    "            result = files\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keywords(text_what,text_from):\n",
    "    \n",
    "    text_what = text_what.split() \n",
    "    text_from = text_from.split() \n",
    "    tokeep = [item for item in text_from if item not in text_what]\n",
    "    \n",
    "    return ' '.join(tokeep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check_int_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_int_addr(addr):\n",
    "    chars = \"áčďéěíňóřšťúůýžÁČĎÉĚÍŇÓŘŠŤÚŮÝŽ\"\n",
    "    return not any(item in chars for item in addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clear_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_a3_codes = [\n",
    "    'afg', 'alb', 'dza', 'and', 'ago', 'arg', 'arm', 'aus', 'aut', 'aze',\n",
    "    'bhs', 'bhr', 'bgd', 'brb', 'blr', 'bel', 'blz', 'ben', 'btn', 'bol',\n",
    "    'bih', 'bwa', 'bra', 'brn', 'bgr', 'bfa', 'bdi', 'cpv', 'khm', 'cmr',\n",
    "    'can', 'caf', 'tcd', 'chl', 'chn', 'col', 'com', 'cog', 'cod', 'cri',\n",
    "    'civ', 'hrv', 'cub', 'cyp', 'cze', 'dnk', 'dji', 'dma', 'dom', 'ecu',\n",
    "    'egy', 'slv', 'gnq', 'eri', 'est', 'swz', 'eth', 'fji', 'fin', 'fra',\n",
    "    'gab', 'gmb', 'geo', 'deu', 'gha', 'grc', 'grd', 'gtm', 'gin', 'gnb',\n",
    "    'guy', 'hti', 'hnd', 'hun', 'isl', 'ind', 'idn', 'irn', 'irq', 'irl',\n",
    "    'isr', 'ita', 'jam', 'jpn', 'jor', 'kaz', 'ken', 'kir', 'prk', 'kor',\n",
    "    'kwt', 'kgz', 'lao', 'lva', 'lbn', 'lso', 'lbr', 'lby', 'lie', 'ltu',\n",
    "    'lux', 'mdg', 'mwi', 'mys', 'mdv', 'mli', 'mlt', 'mhl', 'mrt', 'mus',\n",
    "    'mex', 'fsm', 'mda', 'mco', 'mng', 'mne', 'mar', 'moz', 'mmr', 'nam',\n",
    "    'nru', 'npl', 'nld', 'nzl', 'nic', 'ner', 'nga', 'mkd', 'nor', 'omn',\n",
    "    'pak', 'plw', 'pan', 'png', 'pry', 'per', 'phl', 'pol', 'prt', 'qat',\n",
    "    'rou', 'rus', 'rwa', 'kna', 'lca', 'vct', 'wsm', 'smr', 'stp', 'sau',\n",
    "    'sen', 'srb', 'syc', 'sle', 'sgp', 'svk', 'svn', 'slb', 'som', 'zaf',\n",
    "    'ssd', 'esp', 'lka', 'sdn', 'sur', 'swe', 'che', 'syr', 'twn', 'tjk',\n",
    "    'tza', 'tha', 'tls', 'tgo', 'ton', 'tto', 'tun', 'tur', 'tkm', 'tuv',\n",
    "    'uga', 'ukr', 'are', 'gbr', 'usa', 'ury', 'uzb', 'vut', 'vat', 'ven',\n",
    "    'vnm', 'yem', 'zmb', 'zwe'\n",
    "]\n",
    "\n",
    "iso_a2_codes = [\n",
    "    'af', 'al', 'dz', 'as', 'ad', 'ao', 'ar', 'am', 'aw', 'au', \n",
    "    'at', 'az', 'bs', 'bh', 'bd', 'bb', 'by', 'be', 'bz', 'bj', \n",
    "    'bt', 'bo', 'ba', 'bw', 'br', 'bn', 'bg', 'bf', 'bi', 'kh', \n",
    "    'cm', 'ca', 'cv', 'ky', 'cf', 'td', 'cl', 'cn', 'co', 'km', \n",
    "    'cg', 'cd', 'ck', 'cr', 'ci', 'hr', 'cu', 'cy', 'cz', 'dk', \n",
    "    'dj', 'dm', 'do', 'ec', 'eg', 'sv', 'gq', 'er', 'ee', 'sz', \n",
    "    'et', 'fj', 'fi', 'fr', 'ga', 'gm', 'ge', 'de', 'gh', 'gr', \n",
    "    'gd', 'gt', 'gn', 'gw', 'gy', 'ht', 'hn', 'hu', 'is', 'in', \n",
    "    'id', 'ir', 'iq', 'ie', 'il', 'it', 'jm', 'jp', 'jo', 'kz', \n",
    "    'ke', 'ki', 'kp', 'kr', 'kw', 'kg', 'la', 'lv', 'lb', 'ls', \n",
    "    'lr', 'ly', 'li', 'lt', 'lu', 'mg', 'mw', 'my', 'mv', 'ml', \n",
    "    'mt', 'mh', 'mq', 'mr', 'mu', 'mx', 'fm', 'md', 'mc', 'mn', \n",
    "    'me', 'ma', 'mz', 'mm', 'na', 'nr', 'np', 'nl', 'nz', 'ni', \n",
    "    'ne', 'ng', 'mk', 'no', 'om', 'pk', 'pw', 'pa', 'pg', 'py', \n",
    "    'pe', 'ph', 'pl', 'pt', 'qa', 'ro', 'ru', 'rw', 'kn', 'lc', \n",
    "    'vc', 'ws', 'sm', 'st', 'sa', 'sn', 'rs', 'sc', 'sl', 'sg', \n",
    "    'sk', 'si', 'sb', 'so', 'za', 'ss', 'es', 'lk', 'sd', 'sr', \n",
    "    'se', 'ch', 'sy', 'tw', 'tj', 'tz', 'th', 'tl', 'tg', 'to', \n",
    "    'tt', 'tn', 'tr', 'tm', 'tv', 'ug', 'ua', 'ae', 'gb', 'us', \n",
    "    'uy', 'uz', 'vu', 'va', 've', 'vn', 'ye', 'zm', 'zw'\n",
    "]\n",
    "\n",
    "iso_codes = iso_a3_codes+iso_a2_codes\n",
    "\n",
    "def remove_iso_code(string, iso_codes):\n",
    "    return ' '.join(string.split()[:-1]) if string.split()[-1] in iso_codes else string\n",
    "\n",
    "def remove_duplicate_words(string):\n",
    "    return ' '.join(dict.fromkeys(string.split()))\n",
    "\n",
    "def clear_addr(addr):\n",
    "    retult = pd.NA\n",
    "    \n",
    "    addr = addr.lower()   \n",
    "    addr = unidecode(addr) \n",
    "    addr = remove_duplicate_words(addr)\n",
    "    addr = remove_iso_code(addr, iso_codes)\n",
    "    addr = re.sub(r'\\b\\d{1,4}\\b', '', addr)\n",
    "    addr = re.sub(r'\\b\\w*\\d\\w*\\b', '', addr)\n",
    "\n",
    "    chars_to_replace = ['-', '/']\n",
    "    for char in chars_to_replace: addr = addr.replace(char, ' ')\n",
    "\n",
    "    chars_to_replace = [',','#']\n",
    "    for char in chars_to_replace: addr = addr.replace(char, '')        \n",
    "\n",
    "    addr = addr.split()\n",
    "    \n",
    "    words_to_remove = ['mesto','kraj','psc','id', 'ds','box','suite','room','floor']\n",
    "    addr = [word for word in addr if word not in words_to_remove]\n",
    "\n",
    "    words_with_chars_to_remove = ['@', '(', ')', '.',':', ';', '\\'']\n",
    "    addr = [word for word in addr if all(char not in word for char in words_with_chars_to_remove)]\n",
    "\n",
    "    addr =  ' '.join(addr)    \n",
    "\n",
    "    result = addr\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clear_unused_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_unused_files(files_used):\n",
    "        \n",
    "    files = [f'__documents/{file}' for file in os.listdir('__documents')]\n",
    "    \n",
    "    files_delete = set(files) - set(files_used)\n",
    "    \n",
    "    for file in files_delete:\n",
    "        if os.path.isfile(file):\n",
    "            os.remove(file)\n",
    "        else:\n",
    "            shutil.rmtree(file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frame_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_info(frame,mem=False,dtype=False,sample=False,n=5,width=50,columns=50):\n",
    "\n",
    "    size = len(frame)\n",
    "\n",
    "    if mem: \n",
    "        print(round(frame.memory_usage(deep=True)/(1024**3),1).to_string())\n",
    "        print('Total ',round(frame.memory_usage(deep=True).sum()/(1024**3),1))\n",
    "\n",
    "    if dtype:\n",
    "        print()\n",
    "        print(frame.dtypes.to_string())     \n",
    "\n",
    "    if sample:\n",
    "        frame = frame.sample(min(n,size))\n",
    "\n",
    "    with pd.option_context('display.min_rows', n, 'display.max_rows', n,\n",
    "                           'display.max_columns', columns,'display.max_colwidth', width):\n",
    "        display(frame)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_index = 'https://data.smlouvy.gov.cz/'\n",
    "download_file(url=main_index, local_path='__cache/indexes/main_index.xml', use_cached=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('__cache/indexes/main_index.xml', 'r', encoding='utf-8') as file:\n",
    "    main_index = file.read()\n",
    "\n",
    "main_index = xmltodict.parse(main_index)['index']['dump']\n",
    "main_index = pd.DataFrame(main_index)\n",
    "main_index = main_index.loc[main_index.den.isna()]\n",
    "main_index['url'] = main_index.pop('odkaz')\n",
    "main_index.drop(columns=[col for col in main_index.columns if col != 'url'], inplace=True)\n",
    "\n",
    "main_index['date'] = main_index.url.apply(lambda x: x.split('dump_')[1].split('.')[0])\n",
    "main_index = main_index.sort_values(by ='date').reset_index(drop=True)\n",
    "\n",
    "main_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_index['file'] = main_index.url.apply(lambda x: f'month_index{x.split('dump')[-1]}')\n",
    "main_index.progress_apply(\n",
    "    lambda r: download_file(url=r.url, local_path=f'__cache/indexes/{r.file}',use_cached=True), axis=1)\n",
    "\n",
    "main_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "for item in tqdm(main_index.file.tolist(),file=sys.stdout,ncols=100):\n",
    "\n",
    "    with open(f'__cache/indexes/{item}', 'r', encoding='utf-8') as f:\n",
    "        item_data = f.read()\n",
    "    \n",
    "    # skip potential empty indexes\n",
    "    try:\n",
    "        item_data = xmltodict.parse(item_data)['dump']['zaznam']\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    item_data = pd.DataFrame(item_data)\n",
    "\n",
    "    item_parsed = pd.DataFrame()\n",
    "    \n",
    "    item_parsed['regis_id'] = item_data.identifikator.apply(lambda x: x.get('idSmlouvy',pd.NA))\n",
    "    item_parsed['contr_id'] = item_data.identifikator.apply(lambda x: x.get('idVerze',pd.NA))\n",
    "    item_parsed['date'] = item_data.casZverejneni\n",
    "    item_parsed['valid'] = item_data.platnyZaznam\n",
    "    \n",
    "    item_parsed['party'] = item_data.smlouva.apply(lambda x: x.get('smluvniStrana',pd.NA))\n",
    "    item_parsed['party_name'] = item_parsed.party.apply(\n",
    "        lambda x: x.get('nazev',pd.NA) if isinstance(x, dict) else x[0].get('nazev',pd.NA))    \n",
    "    item_parsed['party_addr'] = item_parsed.party.apply(\n",
    "        lambda x: x.get('adresa',pd.NA) if isinstance(x, dict) else x[0].get('adresa',pd.NA))\n",
    "    item_parsed['party_postbox'] = item_parsed.party.apply(\n",
    "        lambda x: x.get('datovaSchranka','no_postbox') if isinstance(x, dict) else x[0].get('datovaSchranka','no_postbox'))\n",
    "    item_parsed.pop('party')        \n",
    "    \n",
    "    item_parsed['docs_url'] = item_data.prilohy.apply(lambda x: x.get('priloha',pd.NA) if pd.notna(x) else {})\n",
    "    item_parsed['docs_url'] = item_parsed.docs_url.apply(lambda x: parse_archive_files(x))\n",
    "    item_parsed = item_parsed.explode('docs_url',ignore_index = True).rename(columns = {'docs_url':'doc_url'})\n",
    "    item_parsed['doc_id'] = item_parsed.groupby('contr_id').cumcount()+1\n",
    "    # item_parsed['doc_cnt'] = item_parsed.groupby('contr_id').doc_id.transform('max')\n",
    "    item_parsed.insert(2, 'doc_id', item_parsed.pop('doc_id'))\n",
    "    # item_parsed.insert(3, 'doc_cnt', item_parsed.pop('doc_cnt'))\n",
    "    data = pd.concat([data,item_parsed],ignore_index=True)\n",
    "\n",
    "frame_info(data,mem=True,dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data.date.str[:10]\n",
    "data['date'] = pd.to_datetime(data.date,format='%Y-%m-%d')\n",
    "data = data.sort_values(by='date')\n",
    "\n",
    "frame_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear data\n",
    "data['party_int_addr'] = data.party_addr.progress_apply(check_int_addr)\n",
    "todrop = data.loc[(data.valid == '0') | (data.party_postbox != 'no_postbox') | (data.party_int_addr == False)]\n",
    "data.drop(todrop.index,inplace=True)\n",
    "data.pop('party_int_addr'); data.pop('party_postbox'); data.pop('valid')\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear addr\n",
    "data['party_addrclear'] = data.party_addr.progress_apply(clear_addr)\n",
    "data['party_addrclear'] = data.progress_apply(\n",
    "    lambda r: remove_keywords(text_what=r.party_name,text_from=r.party_addrclear),axis=1)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geolocate party\n",
    "data[['party_coords','party_bbox','party_place','party_country']]  = data.progress_apply(\n",
    "    lambda r: geocode_addr(r.party_addrclear,try_nominatim_local=False), axis=1,result_type='expand')\n",
    "\n",
    "todrop = data.loc[data.party_place.isna() | (data.party_country == 'cz')]\n",
    "display(len(todrop))\n",
    "data = data.drop(todrop.index)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop uncertain locations\n",
    "data['party_bbsize'] = data.party_bbox.apply(lambda x: great_circle(x[0],x[1]).m)\n",
    "\n",
    "todrop = data.loc[(data.party_bbsize > 10000) | (data.party_bbsize < 10)]\n",
    "display(len(todrop))\n",
    "data = data.drop(todrop.index)\n",
    "\n",
    "frame_info(data,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check location for party place (street, square)\n",
    "data['valid_loc'] = data.apply(lambda r: r.party_place in unidecode(r.party_addr.lower()),axis=1)\n",
    "\n",
    "todrop = data.loc[(data.valid_loc == False)]\n",
    "display(len(todrop))\n",
    "data = data.drop(todrop.index)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['doc_path'] = data.progress_apply(\n",
    "    lambda r: download_file(url=r.doc_url,local_path=f\"__documents/{r.contr_id}_{r.doc_id}.{r.doc_url.split('.')[-1].lower()}\",\n",
    "                            show=False,use_cached=True), axis=1)\n",
    "\n",
    "todrop = data.loc[data.doc_path.isna()]\n",
    "display(len(todrop))\n",
    "data = data.drop(todrop.index)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['doc_raw'] = data.doc_path.progress_apply(GET_FILE_SOURCE)\n",
    "\n",
    "todrop = data.loc[data.doc_raw.isna()]\n",
    "display(len(todrop))\n",
    "data = data.drop(todrop.index)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['doc_text'] = data.doc_raw.progress_apply(parse_text)\n",
    "\n",
    "todrop = data.loc[data.doc_text.isna()]\n",
    "display(len(todrop))\n",
    "data = data.drop(todrop.index)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['doc_lng'] = data.doc_text.progress_apply(GET_LNG)\n",
    "\n",
    "todrop = data.loc[(data.doc_lng.isna()) | (data.doc_lng != 'EN')]\n",
    "display(len(todrop))\n",
    "data = data.drop(todrop.index)\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc cnt\n",
    "data['doc_cnt'] = data.groupby('contr_id').doc_id.transform('count')\n",
    "data.insert(3, 'doc_cnt', data.pop('doc_cnt'))\n",
    "\n",
    "frame_info(data,mem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear not used documents\n",
    "files_used = data.doc_path.to_list()\n",
    "clear_unused_files(files_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.party_country.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.party_country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.party_coords.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.doc_id.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(data.loc[data.doc_id > 1])/len(data),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.loc[data.doc_text.str.contains('sanction')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.loc[data.doc_text.str.contains('export')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_fl = folium.Map(zoom_start=13, width=600, height=600)\n",
    "\n",
    "uninque_locs = data.drop_duplicates(subset='party_coords')\n",
    "\n",
    "for item in uninque_locs.itertuples():\n",
    "    folium.Marker(location=[float(item.party_coords[0]),float(item.party_coords[1])],\n",
    "                 popup=f'Party: {item.party_name} <br><br> Document: {item.doc_path} <br><br> Date: {item.date}',\n",
    "                lazy=True,\n",
    "                 ).add_to(map_fl)\n",
    "\n",
    "map_fl.save(f'result-figures/map-parties-locations.html')\n",
    "\n",
    "# uncomment here for updated data\n",
    "display(HTML(open(f'result-figures/map-parties-locations.html').read()));\n",
    "print()\n",
    "#Image('result-figures/map-parties-locations-part-static.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo-political example Safran Helicopter, contract id 18923211, document id 2\n",
    "# walk-trough example, Meerstetter Engineering, contract id 32046292, document id 2\n",
    "contract_id = '32046292'\n",
    "document_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contract with documents\n",
    "contract = data.loc[(data.contr_id == contract_id)]\n",
    "contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document\n",
    "document = data.loc[(data.contr_id == contract_id) & (data.doc_id == document_id)]\n",
    "document.doc_text.item()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.pop('doc_text');document.pop('doc_raw')\n",
    "document.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = data[['contr_id','doc_id','doc_cnt','date','party_name','party_coords','party_country',\n",
    "               'doc_path','doc_text']]\n",
    "export.to_csv('result-corpus/corpus.csv', encoding='utf-8', index=False, errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored = pd.read_csv('result-corpus/corpus.csv')\n",
    "\n",
    "frame_info(stored)"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_full_width": true,
  "deepnote_notebook_id": "f1d62ecf8da24069ac0aa12e87e0395e",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
